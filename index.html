<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <title>Anurag Ajay</title>
  
  <meta name="author" content="Anurag Ajay">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Anurag Ajay</name>
              </p>
              <p>I am a Ph.D. student in EECS at <a href="https://csail.mit.edu">MIT CSAIL</a> advised by Professor <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>. I am grateful for support from MIT presidential fellowship.
              </p>
              <p>In Summer 2020, I did a research internship with <a href="https://research.google/people/105364/">Ofir Nachum</a> and <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> at <a href="https://research.google/teams/brain/">Google Brain</a>, working on unsupervised skill discovery for improving offline deep reinforcement learning.                
              </p>
              <p>
               In May 2019, I received my S.M. in EECS from MIT advised by Professor <a href="https://people.csail.mit.edu/lpk/">Leslie Kaelbling</a> and Professor <a href="https://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a>. Before MIT, I completed my bachelor's degree in EECS from <a href="https://eecs.berkeley.edu">UC Berkeley</a> where I worked with Professor <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> and Professor <a href="https://people.eecs.berkeley.edu/svlevine/">Sergey Levine</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:aajay@mit.edu">Email</a> &nbsp/&nbsp
                <a href="data/anurag_resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=uv7g5kMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="http://www.linkedin.com/in/anurag-ajay-3060b080/"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/AnuragAjayLarge.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/AnuragAjay.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>            
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>My research goal is to design algorithms that can enable agents to continuously interact, learn, and perform complex tasks in their environments. To this end, I am interested in learning <strong>Visuomotor Priors for Behavior Transfer</strong>: leveraging past interactions to learn both visual and control priors that allows agent to quickly adapt to new environments.
              </p>
              <p>In the past, I have worked on using deep learning for state estimation, building model-based deep reinforcement learning algorithms for robotics and learning dynamics model, with the help of physics engines, in the robotics domain.               
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/diametr.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=2ovFjGGDFjc">
                <papertitle>Distributionally Adaptive Meta Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Anurag Ajay</strong>,
              <a href="https://dibyaghosh.com/">Dibya Ghosh</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
              <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>,
              <a href="https://abhishekunique.github.io/">Abhishek Gupta</a>
              <br>
              <em>NeurIPS</em>, 2022 (In Submission)
              <br>
              <em>ICML Decision Awareness in Reinforcement Learning Workshop</em>, 2022,
              <br>
              <em>ICML Principles of Distribution Shift Workshop</em>, 2022,
              <br>
              <a href="https://openreview.net/forum?id=2ovFjGGDFjc">openreview</a> /
              <a href="https://openreview.net/forum?id=2ovFjGGDFjc">bibtex</a> /
              <a href="https://openreview.net/forum?id=2ovFjGGDFjc">project page</a>
              <p></p>
              <p> Our framework centers on an adaptive approach to distributional robustness that trains a population of meta-policies to be robust to varying levels of distribution shift. When evaluated on a potentially shifted test-time distribution of tasks, this allows us to choose the meta-policy with the most appropriate level of robustness, and use it to perform fast adaptation. We formally show how our framework allows for improved regret under distribution  shift, and empirically show its efficacy on simulated robotics problems under a wide range of distribution shifts.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/apev.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2207.02200">
                <papertitle>Offline RL Policies Should be Trained to be Adaptive</papertitle>
              </a>
              <br>
              <a href="https://dibyaghosh.com/">Dibya Ghosh</a>,
              <strong>Anurag Ajay</strong>,
              <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em>ICML</em>, 2022 &nbsp <font color="red"><strong>(ICML Long talk)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2207.02200">arXiv</a> /
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:wxCDOExFvlIJ:scholar.google.com/&output=citation&scisdr=CgUBYUkfEJGmmlnvWWM:AAGBfm0AAAAAYs3pQWMNnKDuDyVnJj_8IS3wNqVq3R0D&scisig=AAGBfm0AAAAAYs3pQb_wtZ4Tg45o_HYNK5Jh6Se_fNDb&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>
              <p></p>
              <p> We propose that offline RL methods should instead be adaptive in the presence of uncertainty. We show that acting optimally in offline RL in a Bayesian sense involves solving an implicit POMDP. As a result, optimal policies for offline RL must be adaptive, depending not just on the current state but rather all the transitions seen so far during evaluation.We present a model-free algorithm for approximating this optimal adaptive policy, and demonstrate the efficacy of learning such adaptive policies in offline RL benchmarks.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ff.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=vIC-xLFuM6">
                <papertitle>Overcoming the spectral bias of Neural Value approximation</papertitle>
              </a>
              <br>
              <strong>Anurag Ajay*</strong>,
              <a href="https://www.episodeyang.com/">Ge Yang*</a>,                               
              <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>
              (*Equal Contribution) 
              <br>
              <em>ICLR</em>, 2022
              <br>
              <a href="https://openreview.net/forum?id=vIC-xLFuM6">openreview</a> /
              <a href="https://openreview.net/forum?id=vIC-xLFuM6">bibtex</a> /
              <a href="https://geyang.github.io/ffn">project page</a>
              <p></p>
              <p> We re-examine off-policy reinforcement learning through the lens of kernel regression and propose to overcome such bias via a composite neural tangent kernel. With just a single line-change, our approach, the Fourier feature networks (FFN) produce state-of-the-art performance on challenging continuous control domains with only a fraction of the compute. Faster convergence and better off-policy stability also make it possible to remove the target network without suffering catastrophic divergences, which further reduces TD(0)â€™s bias to overestimate the value.
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/procgen.gif" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1FnqQ0BFQbwANMNN6rm4cJnKoCjs4FL7_/view">
                <papertitle>Understanding the Generalization Gap in Visual Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Anurag Ajay*</strong>,
              <a href="https://www.episodeyang.com/">Ge Yang*</a>,                               
              <a href="https://research.google/people/105364/">Ofir Nachum</a>,
              <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>
              (*Equal Contribution) 
              <br>
              <em>ICML Reinforcement Learning for Real Life Workshop</em>, 2021
              <br>
              <a href="https://drive.google.com/file/d/1FnqQ0BFQbwANMNN6rm4cJnKoCjs4FL7_/view">link</a> /
              <a href="https://openreview.net/forum?id=eqaxDZg4MHw">bibtex</a>
              <p></p>
              <p> We use procedurally generated video games to empirically investigate several hypotheses to explain the lack of transfer. We also show that simple auxiliary tasks can improve the generalization of policies. Contrary to the belief that policy adaptation to new levels requires full policy finetuning, we find that visual features transfer across levels, and only the parameters, that use these visual features to predict actions, require finetuning.
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/opal_kitchen.gif" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2010.13611.pdf">
                <papertitle>OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Anurag Ajay</strong>,
              <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a>,
              <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>, 
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,                                
              <a href="https://research.google/people/105364/">Ofir Nachum</a>
              <br>
              <em>ICLR</em>, 2021 
              <br>
              <a href="https://arxiv.org/abs/2010.13611">arXiv</a> /
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:YmRslK90AbEJ:scholar.google.com/&output=citation&scisdr=CgWycf-5EIKnl0-LKyc:AAGBfm0AAAAAX6iOMyfJY9QWEIeCAVtkUTkP0ttu7fsq&scisig=AAGBfm0AAAAAX6iOM4n43YaYFRdTqI8QwiHo6VMewwCN&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a> /
              <a href="https://sites.google.com/view/opal-iclr">project page</a>
              <p></p>
              <p> When presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. Primitives extracted in this way serve two purposes: they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon yielding better learning in theory, and improved offline RL in practice.
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eap.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://biases-invariances-generalization.github.io/pdf/big_11.pdf">
                <papertitle>Learning Action Priors for Visuomotor transfer</papertitle>
              </a>
              <br>
              <strong>Anurag Ajay</strong>,
              <a href="http://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>,                                              
              <br>
              <em>ICML Inductive Biases, Invariances and Generalization in RL (BIG) Workshop</em>, 2020
              <br>
              <p></p>
              <p> We learn a latent space for past useful action trajectories using an autoencoding model and explore in the learned space which improves performance in the context of transfer learning across tasks and in multi-task reinforcement learning.
              </p>
            </td>
          </tr>          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/point_res.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2009.03994.pdf">
                <papertitle>Long-Horizon Prediction and Uncertainty Propagation with Residual
                Point Contact Learners</papertitle>
              </a>
              <br>              
              <a href="http://nfazeli.mit.edu/">Nima Fazeli</a>,
              <strong>Anurag Ajay</strong>,
              <a href="http://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>
              <br>
              <em>ICRA</em>, 2020 
              <br>
              <a href="https://arxiv.org/abs/2009.03994">arXiv</a> /
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:EqMc5MzIgyEJ:scholar.google.com/&output=citation&scisdr=CgWycf-5EIKnl0-HhE8:AAGBfm0AAAAAX6iCnE97YZhxT6JR0qTeNk6GZpJPVIZg&scisig=AAGBfm0AAAAAX6iCnB6OihFnYDezlPl9OaisSEtOG7HY&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>
              <p></p>
              <p> We propose a self-supervised approach to learning residual models for rigid-body simulators that exploits corrections of contact models to refine predictive performance and propagate uncertainty.
              </p>
            </td>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/skill_hier.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://web.mit.edu/tslvr/www/papers/genplan20_camera_ready.pdf">
                <papertitle>Learning skill hierarchies from predicate descriptions and self-supervision</papertitle>
              </a>
              <br>
              <a href="http://web.mit.edu/tslvr/www/">Tom Silver*</a>,
              <a href="https://rohanchitnis.com/">Rohan Chitnis*</a>, 
              <strong>Anurag Ajay</strong>,
              <a href="https://people.csail.mit.edu/lpk/">Leslie Kaelbling</a>,                                
              <a href="http://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a>               
              <br>
              <em>AAAI GenPlan Workshop</em>, 2020
              <br>
              <p></p>
              <p> We learn lifted, goal-conditioned policies and use STRIPS planning with learned operator descriptions to solve a large suite of unseen test tasks.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/capsule.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744622">
                <papertitle>Learning to Navigate Endoscopic Capsule Robots</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com.hk/citations?hl=en&user=Ba0lRIwAAAAJ&view_op=list_works&sortby=pubdate">Mehmet Turan</a>,
              <a href="https://www.cs.ox.ac.uk/people/yasin.almalioglu/">Yasin Almalioglu</a>,
              <a href="https://www.lsu.edu/eng/mie/people/researchfaculty/gilbert.php">Hunter Gilbert</a>,
              <a href="https://scholar.harvard.edu/faisalmahmood/home">Faisal Mahmood</a>,
              <a href="https://www.bme.jhu.edu/faculty_staff/nicholas-durr-phd/">Nicholas Durr</a>,
              <a href="https://home.isr.uc.pt/~helder/">Helder Araujo</a>,
              <a href="http://scholar.google.com/citations?user=KBeTs3kAAAAJ&hl=en">Alp Eren SarÄ±</a>,
              <strong>Anurag Ajay</strong>,
              <a href="https://scholar.google.com/citations?user=YU4Ce_MAAAAJ&hl=en">Metin Sitti</a>                          
              <br>
              <em>IEEE RAL</em>, 2019
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744622">paper</a> /
              <a href="data/capsule.bib">bibtex</a>              
              <p></p>
              <p> We use deep reinforcement learning algorithms for controlling endoscopic capsule robots
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sain.gif" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1904.06580.pdf">
                <papertitle>Combining Physical Simulators and Object-Based Networks for Control</papertitle>
              </a>
              <br>
              <strong>Anurag Ajay</strong>,
              <a href="http://web.mit.edu/bauza/www/">Maria Bauza</a>,
              <a href="https://jiajunwu.com">Jiajun Wu</a>, 
              <a href="http://nfazeli.mit.edu/">Nima Fazeli</a>,                                
              <a href="http://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a>,
              <a href="http://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
              <a href="https://people.csail.mit.edu/lpk/">Leslie Kaelbling</a> 
              <br>
              <em>ICRA</em>, 2019 
              <br>
              <a href="https://arxiv.org/abs/1904.06580">arXiv</a> /
              <a href="http://sain.csail.mit.edu/bibtex/sain_icra.bib">bibtex</a> /
              <a href="http://sain.csail.mit.edu/">project page</a>
              <p></p>
              <p> We propose a hybrid dynamics model, simulator-augmented interaction networks (SAIN), combining a physics engine with an object-based neural network for dynamics modeling. Compared with existing models that are purely analytical or purely data-driven, our hybrid model captures the dynamics of interacting objects in a more accurate and data-efficient manner.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/res_phys.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1808.03246.pdf">
                <papertitle>Augmenting Physical Simulators with Stochastic Neural Networks:
                Case Study of Planar Pushing and Bouncing</papertitle>
              </a>
              <br>
              <strong>Anurag Ajay</strong>,
              <a href="https://jiajunwu.com">Jiajun Wu</a>, 
              <a href="http://nfazeli.mit.edu/">Nima Fazeli</a>,
              <a href="http://web.mit.edu/bauza/www/">Maria Bauza</a>,
              <a href="https://people.csail.mit.edu/lpk/">Leslie Kaelbling</a>,     
              <a href="http://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a>,
              <a href="http://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>
              <br>
              <em>IROS</em>, 2018 &nbsp <font color="red"><strong>(Best Paper on Cognitive Robotics)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1808.03246">arXiv</a> /
              <a href="http://physplus.csail.mit.edu/bibtex/physplus_iros.bib">bibtex</a> /
              <a href="http://physplus.csail.mit.edu/">project page</a>
              <p></p>
              <p> Combining symbolic, deterministic simulators with learnable, stochastic neural nets provides us with expressiveness, efficiency, and generalizability simultaneously. Our model outperforms both purely analytical and purely learned simulators consistently on real, standard benchmarks.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rgps.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1610.01112.pdf">
                <papertitle>Reset-Free Guided Policy Search: Efficient Deep Reinforcement Learning with Stochastic Initial States</papertitle>
              </a>
              <br>
              William Montgomery*, 
              <strong>Anurag Ajay*</strong>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, 
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://people.eecs.berkeley.edu/svlevine/">Sergey Levine</a>        
              <br>
              <em>ICRA</em>, 2017
              <br>
              <a href="https://arxiv.org/abs/1610.01112">arXiv</a> /
              <a href="data/rgps.bib">bibtex</a> /
              <a href="https://sites.google.com/site/resetfreegps/">project page</a>
              <p></p>
              <p> We present a new guided policy search algorithm that allows the method to be used in domains where the initial conditions are stochastic, which makes the method more applicable to general reinforcement learning problems and improves generalization performance in our robotic manipulation experiments.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bkf.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1605.07148.pdf">
                <papertitle>Backprop KF: Learning Discriminative Deterministic State Estimators</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/tuomas-haarnoja/">Tuomas Haarnoja</a>, <strong>Anurag Ajay</strong>, <a href="https://people.eecs.berkeley.edu/svlevine/">Sergey Levine</a>, <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
              <br>
              <em>NIPS</em>, 2016
              <br>
              <a href="https://arxiv.org/abs/1605.07148">arXiv</a> /
              <a href="data/bkf.bib">bibtex</a>
              <p></p>
              <p>We represent kalman filter as a computational graph and use it in place of recurrent neural networks for data-efficient end to end visual state estimation.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Reviewer for IROS, IEEE-RAL, ICRA, ICML, NeurIPS
              <br>
              uGSI for CS 189 (Introduction to Machine Learning) Fall 2016, Spring 2017
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              <a href="https://github.com/jonbarron/jonbarron_website">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
